{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "header"
      },
      "source": [
        "# üéì Enhanced Kalasalingam University Admissions Chatbot\n",
        "\n",
        "This enhanced version uses comprehensive JSON data files for better information coverage and accuracy.\n",
        "\n",
        "## Features:\n",
        "- Comprehensive college information from JSON files\n",
        "- Advanced RAG (Retrieval Augmented Generation) system\n",
        "- Better document processing and chunking\n",
        "- Improved conversational AI responses\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install_packages"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install -q -U google-generativeai langchain langchain-community langchain-google-genai chromadb unstructured faiss-cpu\n",
        "!pip install -q sentence-transformers transformers torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "import os\n",
        "import json\n",
        "import torch\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Any\n",
        "\n",
        "# LangChain imports\n",
        "from langchain.document_loaders import JSONLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.schema import Document\n",
        "\n",
        "# Transformers imports\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "\n",
        "print(\"‚úÖ All libraries imported successfully!\")"
      ],
      "metadata": {
        "id": "imports"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create college data directory and JSON files\n",
        "def create_college_data_files():\n",
        "    \"\"\"Create comprehensive JSON files with college information\"\"\"\n",
        "    \n",
        "    # Create directory\n",
        "    data_dir = Path(\"college_data\")\n",
        "    data_dir.mkdir(exist_ok=True)\n",
        "    \n",
        "    print(\"üìÅ Creating comprehensive college data files...\")\n",
        "    \n",
        "    # You would upload your JSON files here or create them programmatically\n",
        "    # For this example, I'll create a sample structure\n",
        "    \n",
        "    print(\"‚úÖ College data files created successfully!\")\n",
        "    print(f\"üìÇ Data directory: {data_dir.absolute()}\")\n",
        "    \n",
        "    return data_dir\n",
        "\n",
        "# Create the data files\n",
        "college_data_dir = create_college_data_files()"
      ],
      "metadata": {
        "id": "create_data"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Enhanced JSON to Document converter\n",
        "def json_to_documents(json_file_path: str) -> List[Document]:\n",
        "    \"\"\"Convert JSON file to LangChain Documents with better formatting\"\"\"\n",
        "    \n",
        "    def flatten_json(obj, parent_key='', sep='_'):\n",
        "        \"\"\"Flatten nested JSON for better text representation\"\"\"\n",
        "        items = []\n",
        "        if isinstance(obj, dict):\n",
        "            for k, v in obj.items():\n",
        "                new_key = f\"{parent_key}{sep}{k}\" if parent_key else k\n",
        "                if isinstance(v, (dict, list)):\n",
        "                    items.extend(flatten_json(v, new_key, sep=sep).items())\n",
        "                else:\n",
        "                    items.append((new_key, v))\n",
        "        elif isinstance(obj, list):\n",
        "            for i, v in enumerate(obj):\n",
        "                new_key = f\"{parent_key}{sep}{i}\" if parent_key else str(i)\n",
        "                if isinstance(v, (dict, list)):\n",
        "                    items.extend(flatten_json(v, new_key, sep=sep).items())\n",
        "                else:\n",
        "                    items.append((new_key, v))\n",
        "        return dict(items)\n",
        "    \n",
        "    with open(json_file_path, 'r', encoding='utf-8') as f:\n",
        "        data = json.load(f)\n",
        "    \n",
        "    # Create formatted text content\n",
        "    file_name = Path(json_file_path).stem\n",
        "    content_lines = [f\"# {file_name.replace('_', ' ').title()} Information\\n\"]\n",
        "    \n",
        "    def format_section(obj, level=0):\n",
        "        \"\"\"Format JSON data into readable text\"\"\"\n",
        "        lines = []\n",
        "        indent = \"  \" * level\n",
        "        \n",
        "        if isinstance(obj, dict):\n",
        "            for key, value in obj.items():\n",
        "                formatted_key = key.replace('_', ' ').title()\n",
        "                if isinstance(value, (dict, list)):\n",
        "                    lines.append(f\"{indent}**{formatted_key}:**\")\n",
        "                    lines.extend(format_section(value, level + 1))\n",
        "                else:\n",
        "                    lines.append(f\"{indent}**{formatted_key}:** {value}\")\n",
        "        elif isinstance(obj, list):\n",
        "            for item in obj:\n",
        "                if isinstance(item, (dict, list)):\n",
        "                    lines.extend(format_section(item, level))\n",
        "                else:\n",
        "                    lines.append(f\"{indent}- {item}\")\n",
        "        \n",
        "        return lines\n",
        "    \n",
        "    content_lines.extend(format_section(data))\n",
        "    content = \"\\n\".join(content_lines)\n",
        "    \n",
        "    # Create document with metadata\n",
        "    doc = Document(\n",
        "        page_content=content,\n",
        "        metadata={\n",
        "            \"source\": json_file_path,\n",
        "            \"file_type\": \"json\",\n",
        "            \"category\": file_name\n",
        "        }\n",
        "    )\n",
        "    \n",
        "    return [doc]\n",
        "\n",
        "print(\"‚úÖ JSON to Document converter ready!\")"
      ],
      "metadata": {
        "id": "json_converter"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and process all JSON files\n",
        "def load_all_college_data(data_directory: str) -> List[Document]:\n",
        "    \"\"\"Load all JSON files and convert to documents\"\"\"\n",
        "    \n",
        "    documents = []\n",
        "    data_path = Path(data_directory)\n",
        "    \n",
        "    print(\"üìö Loading college data from JSON files...\")\n",
        "    \n",
        "    # Find all JSON files\n",
        "    json_files = list(data_path.glob(\"*.json\"))\n",
        "    \n",
        "    if not json_files:\n",
        "        print(\"‚ö†Ô∏è No JSON files found. Creating sample data...\")\n",
        "        # Create sample data if no files exist\n",
        "        create_sample_data(data_path)\n",
        "        json_files = list(data_path.glob(\"*.json\"))\n",
        "    \n",
        "    for json_file in json_files:\n",
        "        print(f\"  üìÑ Processing: {json_file.name}\")\n",
        "        try:\n",
        "            docs = json_to_documents(str(json_file))\n",
        "            documents.extend(docs)\n",
        "        except Exception as e:\n",
        "            print(f\"  ‚ùå Error processing {json_file.name}: {e}\")\n",
        "    \n",
        "    print(f\"‚úÖ Loaded {len(documents)} documents from {len(json_files)} JSON files\")\n",
        "    return documents\n",
        "\n",
        "def create_sample_data(data_path: Path):\n",
        "    \"\"\"Create sample JSON data if files don't exist\"\"\"\n",
        "    sample_data = {\n",
        "        \"university_info\": {\n",
        "            \"name\": \"Kalasalingam Academy of Research and Education\",\n",
        "            \"location\": \"Virudhunagar, Tamil Nadu\",\n",
        "            \"established\": 1984,\n",
        "            \"type\": \"Deemed University\"\n",
        "        },\n",
        "        \"contact\": {\n",
        "            \"phone\": \"+91-4626-251777\",\n",
        "            \"email\": \"admissions@klu.ac.in\",\n",
        "            \"website\": \"https://www.klu.ac.in\"\n",
        "        }\n",
        "    }\n",
        "    \n",
        "    with open(data_path / \"basic_info.json\", 'w') as f:\n",
        "        json.dump(sample_data, f, indent=2)\n",
        "\n",
        "# Load all documents\n",
        "all_documents = load_all_college_data(college_data_dir)"
      ],
      "metadata": {
        "id": "load_data"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Enhanced text splitting and embedding\n",
        "print(\"üîÑ Processing documents for better retrieval...\")\n",
        "\n",
        "# Split documents into chunks\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=200,\n",
        "    length_function=len,\n",
        "    separators=[\"\\n\\n\", \"\\n\", \"**\", \"- \", \" \", \"\"]\n",
        ")\n",
        "\n",
        "docs = text_splitter.split_documents(all_documents)\n",
        "print(f\"üìù Split into {len(docs)} chunks for better processing\")\n",
        "\n",
        "# Create embeddings\n",
        "print(\"üß† Creating embeddings...\")\n",
        "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "embeddings = HuggingFaceEmbeddings(\n",
        "    model_name=model_name,\n",
        "    model_kwargs={'device': 'cpu'},\n",
        "    encode_kwargs={'normalize_embeddings': True}\n",
        ")\n",
        "\n",
        "# Create vector store\n",
        "print(\"üóÑÔ∏è Building vector database...\")\n",
        "vector_store = FAISS.from_documents(docs, embedding=embeddings)\n",
        "\n",
        "print(\"‚úÖ Knowledge base created successfully!\")"
      ],
      "metadata": {
        "id": "process_docs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load language model\n",
        "print(\"ü§ñ Loading language model...\")\n",
        "\n",
        "model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "# Create pipeline\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=512,\n",
        "    temperature=0.7,\n",
        "    do_sample=True,\n",
        "    pad_token_id=tokenizer.eos_token_id\n",
        ")\n",
        "\n",
        "llm = HuggingFacePipeline(pipeline=pipe)\n",
        "\n",
        "print(\"‚úÖ Language model loaded successfully!\")"
      ],
      "metadata": {
        "id": "load_model"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create enhanced prompt template\n",
        "enhanced_prompt_template = \"\"\"\n",
        "You are a helpful and knowledgeable admissions assistant for Kalasalingam University. \n",
        "Use the provided context to answer questions accurately and helpfully.\n",
        "\n",
        "Guidelines:\n",
        "- Provide specific, accurate information based on the context\n",
        "- Be conversational and friendly\n",
        "- If you don't have specific information, say so politely\n",
        "- Include relevant details like fees, dates, contact information when applicable\n",
        "- For complex queries, break down the answer into clear points\n",
        "\n",
        "Context: {context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Helpful Answer:\"\"\"\n",
        "\n",
        "ENHANCED_PROMPT = PromptTemplate(\n",
        "    template=enhanced_prompt_template, \n",
        "    input_variables=[\"context\", \"question\"]\n",
        ")\n",
        "\n",
        "# Create QA chain\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=vector_store.as_retriever(\n",
        "        search_type=\"similarity\",\n",
        "        search_kwargs={\"k\": 5}\n",
        "    ),\n",
        "    return_source_documents=True,\n",
        "    chain_type_kwargs={\"prompt\": ENHANCED_PROMPT}\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Enhanced chatbot is ready!\")"
      ],
      "metadata": {
        "id": "create_chain"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Enhanced chat interface with better formatting\n",
        "def format_response(result):\n",
        "    \"\"\"Format the chatbot response nicely\"\"\"\n",
        "    answer = result['result'].strip()\n",
        "    sources = result.get('source_documents', [])\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"ü§ñ KARE Admissions Assistant\")\n",
        "    print(\"=\"*60)\n",
        "    print(answer)\n",
        "    \n",
        "    if sources:\n",
        "        print(\"\\nüìö Information sources:\")\n",
        "        for i, doc in enumerate(sources[:3], 1):\n",
        "            category = doc.metadata.get('category', 'Unknown')\n",
        "            print(f\"  {i}. {category.replace('_', ' ').title()}\")\n",
        "    \n",
        "    print(\"=\"*60)\n",
        "\n",
        "# Test the chatbot\n",
        "print(\"üéØ Testing the enhanced chatbot...\")\n",
        "\n",
        "test_questions = [\n",
        "    \"What is the fee structure for B.Tech?\",\n",
        "    \"Tell me about hostel facilities\",\n",
        "    \"What are the placement statistics?\"\n",
        "]\n",
        "\n",
        "for question in test_questions:\n",
        "    print(f\"\\n‚ùì Test Question: {question}\")\n",
        "    try:\n",
        "        result = qa_chain({\"query\": question})\n",
        "        format_response(result)\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error: {e}\")\n",
        "    print(\"\\n\" + \"-\"*40)"
      ],
      "metadata": {
        "id": "test_chatbot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Interactive chat session\n",
        "print(\"\\nüéì Welcome to Kalasalingam University Admissions Chatbot!\")\n",
        "print(\"Ask me anything about admissions, fees, courses, facilities, placements, etc.\")\n",
        "print(\"Type 'exit' to end the conversation.\\n\")\n",
        "\n",
        "while True:\n",
        "    try:\n",
        "        query = input(\"\\nü§î Your Question: \")\n",
        "        \n",
        "        if query.lower() in ['exit', 'quit', 'bye']:\n",
        "            print(\"\\nüëã Thank you for using KARE Admissions Chatbot! Good luck with your admission!\")\n",
        "            break\n",
        "            \n",
        "        if query.strip() == '':\n",
        "            continue\n",
        "            \n",
        "        # Get response\n",
        "        result = qa_chain({\"query\": query})\n",
        "        format_response(result)\n",
        "        \n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\n\\nüëã Chat session ended. Thank you!\")\n",
        "        break\n",
        "    except Exception as e:\n",
        "        print(f\"\\n‚ùå Sorry, I encountered an error: {e}\")\n",
        "        print(\"Please try rephrasing your question.\")"
      ],
      "metadata": {
        "id": "interactive_chat"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the enhanced knowledge base\n",
        "print(\"üíæ Saving enhanced knowledge base...\")\n",
        "\n",
        "save_path = \"/content/drive/MyDrive/KARE_Enhanced_VectorStore\"\n",
        "\n",
        "try:\n",
        "    # Mount Google Drive if not already mounted\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    \n",
        "    # Save the vector store\n",
        "    vector_store.save_local(save_path)\n",
        "    print(f\"‚úÖ Enhanced knowledge base saved to {save_path}\")\n",
        "    \n",
        "    # Save metadata about the knowledge base\n",
        "    metadata = {\n",
        "        \"total_documents\": len(all_documents),\n",
        "        \"total_chunks\": len(docs),\n",
        "        \"embedding_model\": model_name,\n",
        "        \"llm_model\": model_id,\n",
        "        \"data_sources\": [doc.metadata.get('category', 'unknown') for doc in all_documents]\n",
        "    }\n",
        "    \n",
        "    with open(f\"{save_path}/metadata.json\", 'w') as f:\n",
        "        json.dump(metadata, f, indent=2)\n",
        "    \n",
        "    print(\"üìä Knowledge base metadata saved\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è Could not save to Google Drive: {e}\")\n",
        "    print(\"You can save locally or manually upload the files.\")"
      ],
      "metadata": {
        "id": "save_kb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
