{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "wtLQgYFpNueu"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  DEPRECATION: Building 'langdetect' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'langdetect'. Discussion can be found at https://github.com/pypa/pip/issues/6334\n",
            "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "streamlit 1.48.0 requires requests<3,>=2.27, but you have requests 2.26.0 which is incompatible.\n"
          ]
        }
      ],
      "source": [
        "!pip install -q -U google-generativeai langchain langchain-community langchain-google-genai chromadb unstructured"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qz_IfMplPPjl"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82706dd0"
      },
      "source": [
        "## 3. Load and Process Documents ðŸ“„\n",
        "\n",
        "Now, let's load the documents containing information about Kalasalingam University admissions. This could be one or more PDF files, text files, or other document types.\n",
        "\n",
        "We'll use `unstructured` to load the documents and then split them into smaller chunks for better processing by the language model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cqNCnN3AOfPX",
        "outputId": "d63eb66e-5dbb-47d2-be7f-dd742876873d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Folder 'KalasalingamData_2025' already exists.\n",
            "- File '01_fee_structure_2025.txt' created.\n",
            "- File '02_btech_cse_details_2025.txt' created.\n",
            "- File '03_hostel_information_2025.txt' created.\n",
            "- File '04_important_dates_2025.txt' created.\n",
            "- File '05_common_questions_faq.txt' created.\n",
            "\n",
            "âœ… All data files have been generated successfully!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Define the main data folder\n",
        "folder_name = \"KalasalingamData_2025\"\n",
        "\n",
        "# Create the folder if it doesn't already exist\n",
        "if not os.path.exists(folder_name):\n",
        "    os.makedirs(folder_name)\n",
        "    print(f\"âœ… Folder '{folder_name}' created.\")\n",
        "else:\n",
        "    print(f\"Folder '{folder_name}' already exists.\")\n",
        "\n",
        "# --- Content for the data files ---\n",
        "\n",
        "# File 1: Fee Structure\n",
        "fee_content = \"\"\"\n",
        "# Kalasalingam University - Fee Structure 2025-2026\n",
        "\n",
        "**B.Tech Courses (All Branches):**\n",
        "- Tuition Fee per year: INR 1,60,000\n",
        "- Caution Deposit (Refundable): INR 10,000\n",
        "- Other Fees (Exam, Library, etc.): INR 15,000\n",
        "- Total First Year Fee: INR 1,85,000\n",
        "\n",
        "**Arts & Science (B.Sc / B.Com):**\n",
        "- Tuition Fee per year: INR 50,000\n",
        "- Total First Year Fee: INR 65,000\n",
        "\n",
        "**Note:** Fees are subject to revision. Hostel and mess fees are separate.\n",
        "\"\"\"\n",
        "\n",
        "# File 2: B.Tech CSE Details\n",
        "cse_details_content = \"\"\"\n",
        "# B.Tech - Computer Science and Engineering (CSE) Details 2025\n",
        "\n",
        "**Duration:** 4 Years (8 Semesters)\n",
        "**Eligibility:** A pass in 10+2 (or equivalent) with a minimum of 60% aggregate in Mathematics, Physics, and Chemistry.\n",
        "**Mode of Admission:** Based on scores in KUEE (Kalasalingam University Entrance Exam) or JEE Main.\n",
        "\n",
        "**Key Specializations Offered:**\n",
        "- Artificial Intelligence and Machine Learning\n",
        "- Cybersecurity\n",
        "- Data Science\n",
        "- Cloud Computing\n",
        "\"\"\"\n",
        "\n",
        "# File 3: Hostel Information\n",
        "hostel_content = \"\"\"\n",
        "# Hostel Information 2025-2026\n",
        "\n",
        "**Facilities:**\n",
        "- Separate hostels for boys and girls.\n",
        "- Both AC and Non-AC rooms are available.\n",
        "- 24/7 Wi-Fi connectivity.\n",
        "- In-house laundry service and recreational areas.\n",
        "\n",
        "**Fees (per year):**\n",
        "- Non-AC Room (3-person sharing): INR 65,000\n",
        "- AC Room (3-person sharing): INR 90,000\n",
        "- Mess Fee (Mandatory for all hostel residents): INR 45,000 per year.\n",
        "\"\"\"\n",
        "\n",
        "# File 4: Important Dates\n",
        "# Using today's date (July 29, 2025) for context\n",
        "dates_content = \"\"\"\n",
        "# Important Dates - Admissions 2025\n",
        "\n",
        "**KUEE 2025 (Phase 2):**\n",
        "- Last Date to Apply: August 10, 2025\n",
        "- KUEE Online Entrance Exam: August 18, 2025\n",
        "- Publication of Results: August 22, 2025\n",
        "\n",
        "**Counseling & Admission:**\n",
        "- Counseling for Phase 2: August 25 - August 28, 2025\n",
        "- Last Date for Admission Fee Payment: September 5, 2025\n",
        "- Classes for First Year Begin: September 15, 2025\n",
        "\"\"\"\n",
        "\n",
        "# File 5: Frequently Asked Questions (FAQ)\n",
        "faq_content = \"\"\"\n",
        "# Frequently Asked Questions (FAQ) 2025\n",
        "\n",
        "**Q: Is there a management quota for admission?**\n",
        "A: For details regarding direct admission under management quota, please contact the admissions office directly at +91-XXXXX-XXXXX.\n",
        "\n",
        "**Q: What is the cutoff for CSE based on KUEE rank?**\n",
        "A: The cutoff varies each year. For Phase 1 admissions, the closing rank was around 2500. Phase 2 cutoffs will be determined after the exam.\n",
        "\n",
        "**Q: Can I get an education loan?**\n",
        "A: Yes, the university provides all necessary documentation for students to apply for education loans from nationalized and private banks.\n",
        "\n",
        "**Q: What is the medium of instruction in the classroom?**\n",
        "A: The medium of instruction for all engineering and science courses is English.\n",
        "\"\"\"\n",
        "\n",
        "# --- Dictionary of filenames and their content ---\n",
        "files_to_create = {\n",
        "    \"01_fee_structure_2025.txt\": fee_content,\n",
        "    \"02_btech_cse_details_2025.txt\": cse_details_content,\n",
        "    \"03_hostel_information_2025.txt\": hostel_content,\n",
        "    \"04_important_dates_2025.txt\": dates_content,\n",
        "    \"05_common_questions_faq.txt\": faq_content,\n",
        "}\n",
        "\n",
        "# --- Write the files to the directory ---\n",
        "for filename, content in files_to_create.items():\n",
        "    file_path = os.path.join(folder_name, filename)\n",
        "    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(content.strip())\n",
        "    print(f\"- File '{filename}' created.\")\n",
        "\n",
        "print(\"\\nâœ… All data files have been generated successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P02dO2PoP-8l"
      },
      "outputs": [],
      "source": [
        "!pip install -qU faiss-cpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xCkNGLm5PSO5"
      },
      "outputs": [],
      "source": [
        "#@title Load, Chunk, and Index Your New Files\n",
        "from langchain.document_loaders import DirectoryLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "# Point to the folder with your newly created files\n",
        "data_folder_path = \"KalasalingamData_2025\"\n",
        "\n",
        "print(\"Loading documents from your folder...\")\n",
        "# Load all .txt files\n",
        "loader = DirectoryLoader(data_folder_path, glob=\"**/*.txt\", show_progress=True)\n",
        "documents = loader.load()\n",
        "\n",
        "print(f\"Loaded {len(documents)} documents.\")\n",
        "\n",
        "print(\"\\nSplitting documents into smaller chunks...\")\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)\n",
        "docs = text_splitter.split_documents(documents)\n",
        "print(f\"Split into {len(docs)} chunks.\")\n",
        "\n",
        "print(\"\\nCreating embeddings and indexing in FAISS vector store...\")\n",
        "# This model converts your text chunks into vectors for searching\n",
        "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "embeddings = HuggingFaceEmbeddings(model_name=model_name)\n",
        "\n",
        "# Create the vector store (the AI's knowledge base)\n",
        "vector_store = FAISS.from_documents(docs, embedding=embeddings)\n",
        "\n",
        "print(\"\\nâœ… AI Knowledge Base is built and ready!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qp5DOhFrQcf8"
      },
      "outputs": [],
      "source": [
        "#@title Load an Alternative LLM (TinyLlama) and Create QA Chain\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "print(\"Loading the TinyLlama model... This is a smaller model and should load faster.\")\n",
        "\n",
        "# This is an open-access model that does not require login or agreements.\n",
        "model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "\n",
        "# Create a pipeline to run the model\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=1024,\n",
        "    temperature=0.7,\n",
        ")\n",
        "\n",
        "llm = HuggingFacePipeline(pipeline=pipe)\n",
        "\n",
        "# Create the final QA chain that links the LLM with your data\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=vector_store.as_retriever(search_kwargs={\"k\": 3}),\n",
        "    return_source_documents=True\n",
        ")\n",
        "\n",
        "print(\"\\nâœ… TinyLlama model is loaded! You can now ask questions.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o85atjDBeUr9"
      },
      "outputs": [],
      "source": [
        "#@title Update the AI with a Human-like Personality\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "# --- This is our new set of instructions for the AI ---\n",
        "prompt_template = \"\"\"\n",
        "You are a friendly and helpful admissions assistant for Kalasalingam University.\n",
        "Use the following context to answer the question.\n",
        "Answer directly and concisely, as if you were speaking to someone on the phone.\n",
        "Do not say \"Based on the context\" or \"The context provided shows\". Just give the answer.\n",
        "If you don't know the answer, politely say \"I'm sorry, I don't have that specific information right now, but our admissions office can help.\"\n",
        "\n",
        "Context: {context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Helpful Answer:\"\"\"\n",
        "\n",
        "# --- We create a prompt object from our instructions ---\n",
        "HUMAN_PROMPT = PromptTemplate(\n",
        "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
        ")\n",
        "\n",
        "# --- Re-create the QA chain with our new human-like prompt ---\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=vector_store.as_retriever(),\n",
        "    return_source_documents=True,\n",
        "    chain_type_kwargs={\"prompt\": HUMAN_PROMPT} # This is the key change\n",
        ")\n",
        "\n",
        "print(\"âœ… AI personality updated! It will now sound more human.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pUa3nRz0eut3"
      },
      "outputs": [],
      "source": [
        "#@title Update the AI with a Simpler, Direct Prompt\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "# --- A simpler, more direct set of instructions ---\n",
        "prompt_template = \"\"\"\n",
        "Use the context below to answer the question. Provide a short, direct answer.\n",
        "\n",
        "Context: {context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer:\"\"\"\n",
        "\n",
        "# --- We create a prompt object from our new instructions ---\n",
        "SIMPLE_PROMPT = PromptTemplate(\n",
        "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
        ")\n",
        "\n",
        "# --- Re-create the QA chain with our new simple prompt ---\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=vector_store.as_retriever(),\n",
        "    return_source_documents=True,\n",
        "    chain_type_kwargs={\"prompt\": SIMPLE_PROMPT} # Using the new simple prompt\n",
        ")\n",
        "\n",
        "print(\"âœ… AI instructions have been simplified for more direct answers.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5QoPl_FoafPO"
      },
      "outputs": [],
      "source": [
        "#@title Start Interactive Chat\n",
        "while True:\n",
        "    query = input(\"\\nðŸ¤” Ask a question about Kalasalingam University admissions (or type 'exit'): \")\n",
        "    if query.lower() == 'exit':\n",
        "        break\n",
        "    if query.strip() == '':\n",
        "        continue\n",
        "\n",
        "    # Get the answer from your AI\n",
        "    result = qa_chain({\"query\": query})\n",
        "\n",
        "    # Print the neat answer\n",
        "    print(\"\\nðŸ’¬ Answer:\")\n",
        "    print(result['result'].strip())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nahhHIfSbEL2",
        "outputId": "62b61a44-9e90-4e1a-9b9f-a7153a5e8406"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving the knowledge base...\n",
            "âœ… Knowledge base saved successfully to /content/drive/MyDrive/Kalasalingam_VectorStore\n"
          ]
        }
      ],
      "source": [
        "#@title Save the Knowledge Base to Google Drive\n",
        "print(\"Saving the knowledge base...\")\n",
        "\n",
        "# Define a path in your Google Drive to save the index\n",
        "save_path = \"/content/drive/MyDrive/Kalasalingam_VectorStore\"\n",
        "\n",
        "# Save the FAISS index\n",
        "vector_store.save_local(save_path)\n",
        "\n",
        "print(f\"âœ… Knowledge base saved successfully to {save_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gF5w4hd1f2V7"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
